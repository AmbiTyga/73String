{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "25GB - Colab",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmbiTyga/73String/blob/main/Data%20Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb49crQkUaDi"
      },
      "source": [
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\r\n",
        "import numpy as np\r\n",
        "from time import time\r\n",
        "import argparse"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKPZlz-pUy8N"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip -q"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y5EONb-Txmn"
      },
      "source": [
        "class GloveTokenizer:\r\n",
        "    def __init__(self, filename, unk='<OOV>', pad='<pad>'):\r\n",
        "        self.filename = filename\r\n",
        "        self.unk = unk\r\n",
        "        self.pad = pad\r\n",
        "        self.stoi = dict()\r\n",
        "        self.itos = dict()\r\n",
        "        self.embedding_matrix = list()\r\n",
        "        with open(filename, 'r', encoding='utf8') as f: # Read tokenizer file\r\n",
        "            for i, line in enumerate(f):\r\n",
        "                values = line.split()\r\n",
        "                self.stoi[values[0]] = i\r\n",
        "                self.itos[i] = values[0]\r\n",
        "                self.embedding_matrix.append([float(v) for v in values[1:]])\r\n",
        "        if self.unk is not None: # Add unk token into the tokenizer\r\n",
        "            i += 1\r\n",
        "            self.stoi[self.unk] = i\r\n",
        "            self.itos[i] = self.unk\r\n",
        "            self.embedding_matrix.append(np.random.rand(len(self.embedding_matrix[0])))\r\n",
        "        if self.pad is not None: # Add pad token into the tokenizer\r\n",
        "            i += 1\r\n",
        "            self.stoi[self.pad] = i\r\n",
        "            self.itos[i] = self.pad\r\n",
        "            self.embedding_matrix.append(np.zeros(len(self.embedding_matrix[0])))\r\n",
        "        self.embedding_matrix = np.array(self.embedding_matrix).astype(np.float32) # Convert if from double to float for efficiency\r\n",
        "\r\n",
        "    def encode(self, sentence):\r\n",
        "        if type(sentence) == str:\r\n",
        "            sentence = sentence.split(' ')\r\n",
        "        elif len(sentence): # Convertible to list\r\n",
        "            sentence = list(sentence)\r\n",
        "        else:\r\n",
        "            raise TypeError('sentence should be either a str or a list of str!')\r\n",
        "        encoded_sentence = []\r\n",
        "        for word in sentence:\r\n",
        "            encoded_sentence.append(self.stoi.get(word, self.stoi[self.unk]))\r\n",
        "        return encoded_sentence\r\n",
        "\r\n",
        "    def decode(self, encoded_sentence):\r\n",
        "        try:\r\n",
        "            encoded_sentence = list(encoded_sentence)\r\n",
        "        except Exception as e:\r\n",
        "            print(e)\r\n",
        "            raise TypeError('encoded_sentence should be either a str or a data type that is convertible to list type!')\r\n",
        "        sentence = []\r\n",
        "        for encoded_word in encoded_sentence:\r\n",
        "            sentence.append(self.itos[encoded_word])\r\n",
        "        return sentence\r\n",
        "\r\n",
        "    def embedding(self, encoded_sentence):\r\n",
        "        return self.embedding_matrix[np.array(encoded_sentence)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG-UbL8HZ4kD",
        "outputId": "38baca47-eae6-4ba8-a166-927da1112483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "pd.read_excel('/content/Training_Data.01 (1).xlsx').head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Business Description</th>\n",
              "      <th>Industry Classification Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ADSOUTH PARTNERS, INC.</td>\n",
              "      <td>Adsouth Partners, Inc. provides advertising ag...</td>\n",
              "      <td>Advertising</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Artec Global Media, Inc.</td>\n",
              "      <td>Artec Global Media, Inc., formerly Artec Consu...</td>\n",
              "      <td>Advertising</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Betawave Corp.</td>\n",
              "      <td>Betawave Corporation provides online marketing...</td>\n",
              "      <td>Advertising</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BOSTON OMAHA Corp</td>\n",
              "      <td>Boston Omaha Corporation is engaged in the bus...</td>\n",
              "      <td>Advertising</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bright Mountain Media Inc</td>\n",
              "      <td>Bright Mountain Media, Inc. is a digital media...</td>\n",
              "      <td>Advertising</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Company Name  ... Industry Classification Tag\n",
              "0     ADSOUTH PARTNERS, INC.  ...                 Advertising\n",
              "1   Artec Global Media, Inc.  ...                 Advertising\n",
              "2             Betawave Corp.  ...                 Advertising\n",
              "3          BOSTON OMAHA Corp  ...                 Advertising\n",
              "4  Bright Mountain Media Inc  ...                 Advertising\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5CbKYEsUGyK"
      },
      "source": [
        "class TextLevelGNNDataset(Dataset): # For instantiating train, validation and test dataset\r\n",
        "    def __init__(self, node_sets, neighbor_sets, public_edge_mask, labels):\r\n",
        "        super(TextLevelGNNDataset).__init__()\r\n",
        "        self.node_sets = node_sets\r\n",
        "        self.neighbor_sets = neighbor_sets\r\n",
        "        self.public_edge_mask = public_edge_mask\r\n",
        "        self.labels = labels\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "        return torch.LongTensor(self.node_sets[i]), \\\r\n",
        "               torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1), \\\r\n",
        "               self.public_edge_mask[torch.LongTensor(self.node_sets[i]).unsqueeze(-1).repeat(1, torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1).shape[-1]), torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1)], \\\r\n",
        "               torch.FloatTensor(self.labels[i])\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.labels)\r\n",
        "\r\n",
        "\r\n",
        "class TextLevelGNNDatasetClass: # This class is used to achieve parameters sharing among datasets\r\n",
        "    def __init__(self, train_filename, test_filename, tokenizer, MAX_LENGTH=10, p=2, min_freq=2, train_validation_split=0.8):\r\n",
        "        self.train_filename = train_filename\r\n",
        "        self.test_filename = test_filename\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.MAX_LENGTH = MAX_LENGTH\r\n",
        "        self.p = p\r\n",
        "        self.min_freq = min_freq\r\n",
        "        self.train_validation_split = train_validation_split\r\n",
        "\r\n",
        "        # self.train_data = pd.read_csv(self.train_filename, header=None)\r\n",
        "        # self.test_data = pd.read_csv(self.test_filename, header=None)\r\n",
        "        \r\n",
        "        self.train_data = pd.read_csv(self.train_filename)\r\n",
        "        self.test_data = pd.read_csv(self.test_filename)\r\n",
        "\r\n",
        "\r\n",
        "        self.stoi = {'<OOV>': 0, '<pad>': 1} # Re-index\r\n",
        "        self.itos = {0: '<OOV>', 1: '<pad>'} # Re-index\r\n",
        "        self.vocab_count = len(self.stoi)\r\n",
        "        self.embedding_matrix = None\r\n",
        "        self.label_dict = dict(zip(self.train_data['Industry Classification Tag'].unique(), pd.get_dummies(self.train_data['Industry Classification Tag'].unique()).values.tolist())) # make changes her\r\n",
        "\r\n",
        "        self.train_dataset, self.validation_dataset = random_split(self.train_data.to_numpy(), [int(len(self.train_data) * train_validation_split), len(self.train_data) - int(len(self.train_data) * train_validation_split)])\r\n",
        "        self.test_dataset = self.test_data.to_numpy()\r\n",
        "\r\n",
        "        self.build_vocab() # Based on train_dataset only. Updates self.stoi, self.itos, self.vocab_count and self.embedding_matrix\r\n",
        "\r\n",
        "        self.train_dataset, self.validation_dataset, self.test_dataset, self.edge_stat, self.public_edge_mask = self.prepare_dataset()\r\n",
        "\r\n",
        "    def build_vocab(self):\r\n",
        "        vocab_list = [sentence.split(' ') for _, sentence,_ in self.train_dataset]\r\n",
        "        unique_vocab = []\r\n",
        "        for vocab in vocab_list:\r\n",
        "            unique_vocab.extend(vocab)\r\n",
        "        unique_vocab = list(set(unique_vocab))\r\n",
        "        for vocab in unique_vocab:\r\n",
        "            if vocab in self.tokenizer.stoi.keys():\r\n",
        "                self.stoi[vocab] = self.vocab_count\r\n",
        "                self.itos[self.vocab_count] = vocab\r\n",
        "                self.vocab_count += 1\r\n",
        "        self.embedding_matrix = self.tokenizer.embedding(self.tokenizer.encode(list(self.stoi.keys())))\r\n",
        "\r\n",
        "    def prepare_dataset(self): # will also build self.edge_stat and self.public_edge_mask\r\n",
        "        # preparing self.train_dataset\r\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split('')][:self.MAX_LENGTH] for _, sentence, _ in self.train_dataset] # Only retrieve the first MAX_LENGTH words in each document\r\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\r\n",
        "        labels = [self.label_dict[label] for _,_,label in self.train_dataset]\r\n",
        "\r\n",
        "        # Construct edge statistics and public edge mask\r\n",
        "        edge_stat, public_edge_mask = self.build_public_edge_mask(node_sets, neighbor_sets, min_freq=self.min_freq)\r\n",
        "        \r\n",
        "        train_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)\r\n",
        "\r\n",
        "        # preparing self.validation_dataset\r\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence, _ in self.validation_dataset] # Only retrieve the first MAX_LENGTH words in each document\r\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\r\n",
        "        labels = [self.label_dict[label] for _,_,label in self.validation_dataset]\r\n",
        "        validation_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)\r\n",
        "\r\n",
        "        # preparing self.test_dataset\r\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence, _ in self.test_dataset] # Only retrieve the first MAX_LENGTH words in each document\r\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\r\n",
        "        labels = [self.label_dict[label] for _,_,label in self.test_dataset]\r\n",
        "        test_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)\r\n",
        "\r\n",
        "        return train_dataset, validation_dataset, test_dataset, edge_stat, public_edge_mask\r\n",
        "\r\n",
        "    def build_public_edge_mask(self, node_sets, neighbor_sets, min_freq=2):\r\n",
        "        edge_stat = torch.zeros(self.vocab_count, self.vocab_count)\r\n",
        "        for node_set, neighbor_set in zip(node_sets, neighbor_sets):\r\n",
        "            for neighbor in neighbor_set:\r\n",
        "                for to_node in neighbor:\r\n",
        "                    edge_stat[node_set, to_node] += 1\r\n",
        "        public_edge_mask = edge_stat < min_freq # mark True at uncommon edges\r\n",
        "        return edge_stat, public_edge_mask\r\n",
        "\r\n",
        "\r\n",
        "def create_neighbor_set(node_set, p=2):\r\n",
        "    if type(node_set[0]) != int:\r\n",
        "        raise ValueError('node_set should be a 1D list!')\r\n",
        "    if p < 0:\r\n",
        "        raise ValueError('p should be an integer >= 0!')\r\n",
        "    sequence_length = len(node_set)\r\n",
        "    neighbor_set = []\r\n",
        "    for i in range(sequence_length):\r\n",
        "        neighbor = []\r\n",
        "        for j in range(-p, p+1):\r\n",
        "            if 0 <= i + j < sequence_length:\r\n",
        "                neighbor.append(node_set[i+j])\r\n",
        "        neighbor_set.append(neighbor)\r\n",
        "    return neighbor_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-yIjfKMUSKv"
      },
      "source": [
        "def pad_custom_sequence(sequences):\r\n",
        "    '''\r\n",
        "    To pad different sequences into a padded tensor for training. The main purpose of this function is to separate different sequence, pad them in different ways and return padded sequences.\r\n",
        "    Input:\r\n",
        "        sequences <list>: A sequence with a length of 4, representing the node sets sequence in index 0, neighbor sets sequence in index 1, public edge mask sequence in index 2 and label sequence in index 3.\r\n",
        "                          And the length of each sequences are same as the batch size.\r\n",
        "                          sequences: [node_sets_sequence, neighbor_sets_sequence, public_edge_mask_sequence, label_sequence]\r\n",
        "    Return:\r\n",
        "        node_sets_sequence <torch.LongTensor>: The padded node sets sequence (works with batch_size >= 1).\r\n",
        "        neighbor_sets_sequence <torch.LongTensor>: The padded neighbor sets sequence (works with batch_size >= 1).\r\n",
        "        public_edge_mask_sequence <torch.BoolTensor>: The padded public edge mask sequence (works with batch_size >= 1).\r\n",
        "        label_sequence <torch.FloatTensor>: The padded label sequence (works with batch_size >= 1).\r\n",
        "    '''\r\n",
        "    node_sets_sequence = []\r\n",
        "    neighbor_sets_sequence = []\r\n",
        "    public_edge_mask_sequence = []\r\n",
        "    label_sequence = []\r\n",
        "    for node_sets, neighbor_sets, public_edge_mask, label in sequences:\r\n",
        "        node_sets_sequence.append(node_sets)\r\n",
        "        neighbor_sets_sequence.append(neighbor_sets)\r\n",
        "        public_edge_mask_sequence.append(public_edge_mask)\r\n",
        "        label_sequence.append(label)\r\n",
        "    node_sets_sequence = torch.nn.utils.rnn.pad_sequence(node_sets_sequence, batch_first=True, padding_value=1)\r\n",
        "    neighbor_sets_sequence, _ = padding_tensor(neighbor_sets_sequence)\r\n",
        "    public_edge_mask_sequence, _ = padding_tensor(public_edge_mask_sequence)\r\n",
        "    label_sequence = torch.nn.utils.rnn.pad_sequence(label_sequence, batch_first=True, padding_value=1)\r\n",
        "    return node_sets_sequence, neighbor_sets_sequence, public_edge_mask_sequence, label_sequence\r\n",
        "\r\n",
        "\r\n",
        "def padding_tensor(sequences, padding_idx=1):\r\n",
        "    '''\r\n",
        "    To pad tensor of different shape to be of the same shape, i.e. padding [tensor.rand(2, 3), tensor.rand(3, 5)] to a shape (2, 3, 5), where 0th dimension is batch_size, 1st and 2nd dimensions are padded.\r\n",
        "    Input:\r\n",
        "        sequences <list>: A list of tensors\r\n",
        "        padding_idx <int>: The index that corresponds to the padding index\r\n",
        "    Return:\r\n",
        "        out_tensor <torch.tensor>: The padded tensor\r\n",
        "        mask <torch.tensor>: A boolean torch tensor where 1 (represents '<pad>') are marked as true\r\n",
        "    '''\r\n",
        "    num = len(sequences)\r\n",
        "    max_len_0 = max([s.shape[0] for s in sequences])\r\n",
        "    max_len_1 = max([s.shape[1] for s in sequences])\r\n",
        "    out_dims = (num, max_len_0, max_len_1)\r\n",
        "    out_tensor = sequences[0].data.new(*out_dims).fill_(padding_idx)\r\n",
        "    for i, tensor in enumerate(sequences):\r\n",
        "        len_0 = tensor.size(0)\r\n",
        "        len_1 = tensor.size(1)\r\n",
        "        out_tensor[i, :len_0, :len_1] = tensor\r\n",
        "    mask = out_tensor == padding_idx # Marking all places with padding_idx as mask\r\n",
        "    return out_tensor, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB1wKg7JUN5Y"
      },
      "source": [
        "\r\n",
        "class MessagePassing(nn.Module):\r\n",
        "    def __init__(self, vertice_count, input_size, out_size, dropout_rate=0, padding_idx=1):\r\n",
        "        super(MessagePassing, self).__init__()\r\n",
        "        self.vertice_count = vertice_count # |V|\r\n",
        "        self.input_size = input_size # d\r\n",
        "        self.out_size = out_size # c\r\n",
        "        self.dropout_rate = dropout_rate\r\n",
        "        self.padding_idx = padding_idx\r\n",
        "        self.information_rate = nn.Parameter(torch.rand(self.vertice_count, 1)) # (|V|, 1), which means it is a column vector\r\n",
        "        self.linear = nn.Linear(self.input_size, self.out_size) # (d, c)\r\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\r\n",
        "\r\n",
        "    def forward(self, node_sets, embedded_node, edge_weight, embedded_neighbor_node):\r\n",
        "        # node_sets: (batch_size, l)\r\n",
        "        # embedded_node: (batch_size, l, d)\r\n",
        "        # edge_weight: (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "        # embedded_neighbor_node: (batch_size, max_sentence_length, max_neighbor_count, d)\r\n",
        "\r\n",
        "        tmp_tensor = (edge_weight.view(-1, 1) * embedded_neighbor_node.view(-1, self.input_size)).view(embedded_neighbor_node.shape) # (batch_size, max_sentence_length, max_neighbor_count, d)\r\n",
        "        tmp_tensor = tmp_tensor.masked_fill(tmp_tensor == 0, -1e18) # (batch_size, max_sentence_length, max_neighbor_count, d), mask for M such that masked places are marked as -1e18\r\n",
        "        tmp_tensor = self.dropout(tmp_tensor)\r\n",
        "        M = tmp_tensor.max(dim=2)[0] # (batch_size, max_sentence_length, d), which is same shape as embedded_node (batch_size, l, d)\r\n",
        "        information_rate = self.information_rate[node_sets] # (batch_size, l, 1)\r\n",
        "        information_rate = information_rate.masked_fill((node_sets == self.padding_idx).unsqueeze(-1), 1) # (batch_size, l, 1), Fill the information rate of the padding index as 1, such that new e_n = (1-i_r) * M + i_r * e_n = (1-1) * 0 + 1 * e_n = e_n (no update)\r\n",
        "        embedded_node = (1 - information_rate) * M + information_rate * embedded_node # (batch_size, l, d)\r\n",
        "        sum_embedded_node = embedded_node.sum(dim=1) # (batch_size, d)\r\n",
        "        x = F.relu(self.linear(sum_embedded_node)) # (batch_size, c)\r\n",
        "#         x = self.dropout(x) # if putting dropout with p=0.5 here, it is equivalent to wiping 4 choices out of 8 choices on the question sheet, which does not make sense. If a dropout layer is placed at here, it works the best when p=0 (disabled), followed by p=0.05, ..., p=0.5 (worst and does not even converge).\r\n",
        "        y = F.softmax(x, dim=1) # (batch_size, c) along the c dimension\r\n",
        "        return y\r\n",
        "\r\n",
        "\r\n",
        "class TextLevelGNN(nn.Module):\r\n",
        "    def __init__(self, pretrained_embeddings, out_size=8, dropout_rate=0, padding_idx=1):\r\n",
        "        super(TextLevelGNN, self).__init__()\r\n",
        "        self.out_size = out_size # c\r\n",
        "        self.padding_idx = padding_idx\r\n",
        "        self.weight_matrix = nn.Parameter(torch.randn(pretrained_embeddings.shape[0], pretrained_embeddings.shape[0])) # (|V|, |V|)        \r\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False, padding_idx=self.padding_idx) # (|V|, d)\r\n",
        "        self.message_passing = MessagePassing(vertice_count=pretrained_embeddings.shape[0], input_size=pretrained_embeddings.shape[1], out_size=self.out_size, dropout_rate=dropout_rate, padding_idx=self.padding_idx) # input_size: (d,); out_size: (c,)\r\n",
        "        self.public_edge_weight = nn.Parameter(torch.randn(1, 1)) # (1, 1)\r\n",
        "\r\n",
        "    def forward(self, node_sets, neighbor_sets, public_edge_mask):\r\n",
        "        # node_sets: (batch_size, l)\r\n",
        "        # neighbor_sets: (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "        # neighbor_sets_mask: (batch_size, max_sentence_length, max_neighbor_count) (no need)\r\n",
        "        # public_edge_mask: (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "\r\n",
        "        embedded_node = self.embedding(node_sets) # (batch_size, l, d)\r\n",
        "        edge_weight = model.weight_matrix[node_sets.unsqueeze(2).repeat(1, 1, neighbor_sets.shape[-1]), neighbor_sets] # (batch_size, max_sentence_length, max_neighbor_count), neighbor_sets.shape[-1]: eg p=2, this expression=5; p=3, this expression=7. This is to first make node_sets to have same shape with neighbor_sets, then just do 1 query instead of 32*100 queries to speed up performance\r\n",
        "        a = edge_weight * ~public_edge_mask # (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "        b = self.public_edge_weight.unsqueeze(2).expand(1, public_edge_mask.shape[-2], public_edge_mask.shape[-1]) * public_edge_mask # (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "        edge_weight = a + b # (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "        embedded_neighbor_node = self.embedding(neighbor_sets) # (batch_size, max_sentece_length, max_neighbor_count, d)\r\n",
        "\r\n",
        "        # Apply mask to edge_weight, to mask and cut-off any relationships to the padding nodes\r\n",
        "        edge_weight = edge_weight.masked_fill((node_sets.unsqueeze(2).repeat(1, 1, neighbor_sets.shape[-1]) == self.padding_idx) | (neighbor_sets == self.padding_idx), 0) # (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "        x = self.message_passing(node_sets, embedded_node, edge_weight, embedded_neighbor_node) # (batch_size, c)\r\n",
        "        return x\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrY0ZJs7Tswj"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument('--cuda', default='0', type=str, required=False,\r\n",
        "                    help='Choosing which cuda to use')\r\n",
        "parser.add_argument('--embedding_size', default=300, type=int, required=False,\r\n",
        "                    help='Number of hidden units in each layer of the graph embedding part')\r\n",
        "parser.add_argument('--p', default=3, type=int, required=False,\r\n",
        "                    help='The window size')\r\n",
        "parser.add_argument('--min_freq', default=2, type=int, required=False,\r\n",
        "                    help='The minimum no. of occurrence for a word to be considered as a meaningful word. Words with less than this occurrence will be mapped to a globally shared embedding weight (to the <unk> token). It corresponds to the parameter k in the original paper.')\r\n",
        "parser.add_argument('--max_length', default=70, type=int, required=False,\r\n",
        "                    help='The max length of each document to be processed')\r\n",
        "parser.add_argument('--dropout', default=0, type=float, required=False,\r\n",
        "                    help='Dropout rate')\r\n",
        "parser.add_argument('--lr', default=1e-3, type=float, required=False,\r\n",
        "                    help='Initial learning rate')\r\n",
        "parser.add_argument('--lr_decay_factor', default=0.9, type=float, required=False,\r\n",
        "                    help='Multiplicative factor of learning rate decays')\r\n",
        "parser.add_argument('--lr_decay_every', default=5, type=int, required=False,\r\n",
        "                    help='Decaying learning rate every ? epochs')\r\n",
        "parser.add_argument('--weight_decay', default=1e-4, type=float, required=False,\r\n",
        "                    help='Weight decay (L2 penalty)')\r\n",
        "parser.add_argument('--warm_up_epoch', default=0, type=int, required=False,\r\n",
        "                    help='Pretraining for ? epochs before early stopping to be in effect')\r\n",
        "parser.add_argument('--early_stopping_patience', default=10, type=int, required=False,\r\n",
        "                    help='Waiting for ? more epochs after the best epoch to see any further improvements')\r\n",
        "parser.add_argument('--early_stopping_criteria', default='loss', type=str, required=False,\r\n",
        "                    choices=['accuracy', 'loss'],\r\n",
        "                    help='Early stopping according to validation accuracy or validation loss')\r\n",
        "parser.add_argument(\"--epoch\", default=300, type=int, required=False,\r\n",
        "                    help='Number of epochs to train')\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "### Fetch data (Run this part to download data for the first time)\r\n",
        "# train_url = 'https://www.cs.umb.edu/~smimarog/textmining/datasets/r8-train-all-terms.txt'\r\n",
        "# test_url = 'https://www.cs.umb.edu/~smimarog/textmining/datasets/r8-test-all-terms.txt'\r\n",
        "\r\n",
        "# pd.read_csv(train_url, sep='\\t').to_csv('r8-train-all-terms.csv', index=False)\r\n",
        "# pd.read_csv(test_url, sep='\\t').to_csv('r8-test-all-terms.csv', index=False)\r\n",
        "###\r\n",
        "\r\n",
        "tokenizer = GloveTokenizer(f'embeddings/glove.6B.{args.embedding_size}d.txt')\r\n",
        "dataset = TextLevelGNNDatasetClass(train_filename='r8-train-all-terms.csv',\r\n",
        "                                   test_filename='r8-test-all-terms.csv',\r\n",
        "                                   train_validation_split=0.8,\r\n",
        "                                   tokenizer=tokenizer,\r\n",
        "                                   p=args.p,\r\n",
        "                                   min_freq=args.min_freq,\r\n",
        "                                   MAX_LENGTH=args.max_length)\r\n",
        "train_loader = DataLoader(dataset.train_dataset, batch_size=32, shuffle=True, collate_fn=pad_custom_sequence)\r\n",
        "validation_loader = DataLoader(dataset.validation_dataset, batch_size=32, shuffle=True, collate_fn=pad_custom_sequence)\r\n",
        "test_loader = DataLoader(dataset.test_dataset, batch_size=32, shuffle=True, collate_fn=pad_custom_sequence)\r\n",
        "\r\n",
        "device = torch.device(f'cuda:{args.cuda}') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "model = TextLevelGNN(pretrained_embeddings=torch.tensor(dataset.embedding_matrix), dropout_rate=args.dropout).to(device)\r\n",
        "criterion = nn.BCELoss()\r\n",
        "\r\n",
        "lr = args.lr\r\n",
        "lr_decay_factor = args.lr_decay_factor\r\n",
        "lr_decay_every = args.lr_decay_every\r\n",
        "weight_decay = args.weight_decay\r\n",
        "\r\n",
        "warm_up_epoch = args.warm_up_epoch\r\n",
        "early_stopping_patience = args.early_stopping_patience\r\n",
        "early_stopping_criteria = args.early_stopping_criteria\r\n",
        "best_epoch = 0 # Initialize\r\n",
        "\r\n",
        "training = {}\r\n",
        "validation = {}\r\n",
        "testing = {}\r\n",
        "training['accuracy'] = []\r\n",
        "training['loss'] = []\r\n",
        "validation['accuracy'] = []\r\n",
        "validation['loss'] = []\r\n",
        "testing['accuracy'] = []\r\n",
        "testing['loss'] = []\r\n",
        "\r\n",
        "for epoch in range(args.epoch):\r\n",
        "    model.train()\r\n",
        "    train_loss = 0\r\n",
        "    train_correct_items = 0\r\n",
        "    previous_epoch_timestamp = time()\r\n",
        "\r\n",
        "    if epoch % lr_decay_every == 0: # Update optimizer for every lr_decay_every epochs\r\n",
        "        if epoch != 0: # When it is the first epoch, disable the lr_decay_factor\r\n",
        "            lr *= lr_decay_factor\r\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\r\n",
        "\r\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(train_loader):\r\n",
        "#         print('Finished batch:', i)\r\n",
        "        node_sets = node_sets.to(device)\r\n",
        "        neighbor_sets = neighbor_sets.to(device)\r\n",
        "        public_edge_masks = public_edge_masks.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)\r\n",
        "        loss = criterion(prediction, labels).to(device)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        train_loss += loss.item()\r\n",
        "        train_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\r\n",
        "    train_accuracy = train_correct_items / len(dataset.train_dataset)\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    validation_loss = 0\r\n",
        "    validation_correct_items = 0\r\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(validation_loader):\r\n",
        "        node_sets = node_sets.to(device)\r\n",
        "        neighbor_sets = neighbor_sets.to(device)\r\n",
        "        public_edge_masks = public_edge_masks.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)\r\n",
        "        loss = criterion(prediction, labels).to(device)\r\n",
        "        validation_loss += loss.item()\r\n",
        "        validation_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\r\n",
        "    validation_accuracy = validation_correct_items / len(dataset.validation_dataset)\r\n",
        "\r\n",
        "#     model.eval()\r\n",
        "    test_loss = 0\r\n",
        "    test_correct_items = 0\r\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(test_loader):\r\n",
        "        node_sets = node_sets.to(device)\r\n",
        "        neighbor_sets = neighbor_sets.to(device)\r\n",
        "        public_edge_masks = public_edge_masks.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)\r\n",
        "        loss = criterion(prediction, labels).to(device)\r\n",
        "        test_loss += loss.item()\r\n",
        "        test_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\r\n",
        "    test_accuracy = test_correct_items / len(dataset.test_dataset)\r\n",
        "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}, Testing Loss: {test_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {validation_accuracy:.4f}, Testing Accuracy: {test_accuracy:.4f}, Time Used: {time()-previous_epoch_timestamp:.2f}s')\r\n",
        "    training['accuracy'].append(train_accuracy)\r\n",
        "    training['loss'].append(train_loss)\r\n",
        "    validation['accuracy'].append(validation_accuracy)\r\n",
        "    validation['loss'].append(validation_loss)\r\n",
        "    testing['accuracy'].append(test_accuracy)\r\n",
        "    testing['loss'].append(test_loss)\r\n",
        "\r\n",
        "    # add warmup mechanism for warm_up_epoch epochs\r\n",
        "    if epoch >= warm_up_epoch:\r\n",
        "        best_epoch = warm_up_epoch\r\n",
        "        # early stopping\r\n",
        "        if early_stopping_criteria == 'accuracy':\r\n",
        "            if validation['accuracy'][epoch] > validation['accuracy'][best_epoch]:\r\n",
        "                best_epoch = epoch\r\n",
        "            elif epoch >= best_epoch + early_stopping_patience:\r\n",
        "                print(f'Early stopping... (No further increase in validation accuracy) for consecutive {early_stopping_patience} epochs.')\r\n",
        "                break\r\n",
        "        if early_stopping_criteria == 'loss':\r\n",
        "            if validation['loss'][epoch] < validation['loss'][best_epoch]:\r\n",
        "                best_epoch = epoch\r\n",
        "            elif epoch >= best_epoch + early_stopping_patience:\r\n",
        "                print(f'Early stopping... (No further decrease in validation loss) for consecutive {early_stopping_patience} epochs.')\r\n",
        "                break\r\n",
        "    elif epoch + 1 == warm_up_epoch:\r\n",
        "        print('--- Warm up finished ---')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}